{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/Chen/Developer/Repository/Clevo-Emotion-Detection-Service/serverless/dev'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''Trains a simple convnet on the MNIST dataset.\n",
    "Gets to 99.25% test accuracy after 12 epochs\n",
    "(there is still a lot of margin for parameter tuning).\n",
    "16 seconds per epoch on a GRID K520 GPU.\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "batch_size = 128\n",
    "# num_classes = 10\n",
    "epochs = 12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "mono=True\n",
    "\n",
    "\n",
    "def get_class_names(path=\"Preproc/\"):  # class names are subdirectory names in Preproc/ directory\n",
    "    class_names = os.listdir(path)\n",
    "    class_names.remove(\".DS_Store\")\n",
    "    return class_names\n",
    "\n",
    "def get_total_files(path=\"Preproc/\",train_percentage=0.8): \n",
    "    sum_total = 0\n",
    "    sum_train = 0\n",
    "    sum_test = 0\n",
    "    subdirs = os.listdir(path)\n",
    "    subdirs.remove(\".DS_Store\")\n",
    "\n",
    "    for subdir in subdirs:\n",
    "        files = os.listdir(path+subdir)\n",
    "        # files.remove(\".DS_Store\")\n",
    "        n_files = len(files)\n",
    "        sum_total += n_files\n",
    "        n_train = int(train_percentage*n_files)\n",
    "        n_test = n_files - n_train\n",
    "        sum_train += n_train\n",
    "        sum_test += n_test\n",
    "    return sum_total, sum_train, sum_test\n",
    "\n",
    "def get_sample_dimensions(path='Preproc/'):\n",
    "    classnames = os.listdir(path)\n",
    "    classnames.remove(\".DS_Store\")\n",
    "    classname = classnames[0]\n",
    "    files = os.listdir(path+classname)\n",
    "    # files.remove(\".DS_Store\")\n",
    "    infilename = files[0]\n",
    "    audio_path = path + classname + '/' + infilename\n",
    "    \n",
    "    print(\"os.path.exists(audio_path)\", os.path.exists(audio_path))\n",
    "    \n",
    "    melgram = np.load(audio_path)\n",
    "    print(\"   get_sample_dimensions: melgram.shape = \",melgram.shape)\n",
    "    return melgram.shape\n",
    " \n",
    "\n",
    "def encode_class(class_name, class_names):  # makes a \"one-hot\" vector for each class name called\n",
    "    try:\n",
    "        idx = class_names.index(class_name)\n",
    "        vec = np.zeros(len(class_names))\n",
    "        vec[idx] = 1\n",
    "        return vec\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def shuffle_XY_paths(X,Y,paths):   # generates a randomized order, keeping X&Y(&paths) together\n",
    "    assert (X.shape[0] == Y.shape[0] )\n",
    "    idx = np.array(range(Y.shape[0]))\n",
    "    np.random.shuffle(idx)\n",
    "    newX = np.copy(X)\n",
    "    newY = np.copy(Y)\n",
    "    newpaths = paths\n",
    "    for i in range(len(idx)):\n",
    "        newX[i] = X[idx[i],:,:]\n",
    "        newY[i] = Y[idx[i],:]\n",
    "        newpaths[i] = paths[idx[i]]\n",
    "    return newX, newY, newpaths\n",
    "\n",
    "\n",
    "'''\n",
    "So we make the training & testing datasets here, and we do it separately.\n",
    "Why not just make one big dataset, shuffle, and then split into train & test?\n",
    "because we want to make sure statistics in training & testing are as similar as possible\n",
    "'''\n",
    "def build_datasets(train_percentage=0.8, preproc=False):\n",
    "    if (preproc):\n",
    "        path = \"Preproc/\"\n",
    "    else:\n",
    "        path = \"Samples/\"\n",
    "\n",
    "    class_names = get_class_names(path=path)\n",
    "    print(\"class_names = \",class_names)\n",
    "\n",
    "    total_files, total_train, total_test = get_total_files(path=path, train_percentage=train_percentage)\n",
    "    print(\"total files = \",total_files)\n",
    "\n",
    "    nb_classes = len(class_names)\n",
    "\n",
    "    # pre-allocate memory for speed (old method used np.concatenate, slow)\n",
    "    mel_dims = get_sample_dimensions(path=path)  # Find out the 'shape' of each data file\n",
    "    X_train = np.zeros((total_train, mel_dims[1], mel_dims[2], mel_dims[3]))   \n",
    "    Y_train = np.zeros((total_train, nb_classes))  \n",
    "    X_test = np.zeros((total_test, mel_dims[1], mel_dims[2], mel_dims[3]))  \n",
    "    Y_test = np.zeros((total_test, nb_classes))  \n",
    "    paths_train = []\n",
    "    paths_test = []\n",
    "\n",
    "    train_count = 0\n",
    "    test_count = 0\n",
    "    for idx, classname in enumerate(class_names):\n",
    "        this_Y = np.array(encode_class(classname,class_names) )\n",
    "        this_Y = this_Y[np.newaxis,:]\n",
    "        class_files = os.listdir(path+classname)\n",
    "        # class_files.remove(\".DS_Store\")\n",
    "        n_files = len(class_files)\n",
    "        n_load =  n_files\n",
    "        n_train = int(train_percentage * n_load)\n",
    "        printevery = 100\n",
    "        print(\"\")\n",
    "        for idx2, infilename in enumerate(class_files[0:n_load]):          \n",
    "            audio_path = path + classname + '/' + infilename\n",
    "            if (0 == idx2 % printevery):\n",
    "                print('\\r Loading class: {:14s} ({:2d} of {:2d} classes)'.format(classname,idx+1,nb_classes),\n",
    "                       \", file \",idx2+1,\" of \",n_load,\": \",audio_path,sep=\"\")\n",
    "            #start = timer()\n",
    "            if (preproc):\n",
    "                melgram = np.load(audio_path)\n",
    "                sr = 8000                \n",
    "#                 sr = 44100\n",
    "            else:\n",
    "              aud, sr = librosa.load(audio_path, mono=mono,sr=None)\n",
    "              melgram = librosa.logamplitude(librosa.feature.melspectrogram(aud, sr=sr, n_mels=96),ref_power=1.0)[np.newaxis,np.newaxis,:,:]\n",
    "\n",
    "            melgram = melgram[:,:,:,0:mel_dims[3]]   # just in case files are differnt sizes: clip to first file size\n",
    "       \n",
    "            #end = timer()\n",
    "            #print(\"time = \",end - start) \n",
    "            if (idx2 < n_train):\n",
    "                # concatenate is SLOW for big datasets; use pre-allocated instead\n",
    "                #X_train = np.concatenate((X_train, melgram), axis=0)  \n",
    "                #Y_train = np.concatenate((Y_train, this_Y), axis=0)\n",
    "                X_train[train_count,:,:] = melgram\n",
    "                Y_train[train_count,:] = this_Y\n",
    "                paths_train.append(audio_path)     # list-appending is still fast. (??)\n",
    "                train_count += 1\n",
    "            else:\n",
    "                X_test[test_count,:,:] = melgram\n",
    "                Y_test[test_count,:] = this_Y\n",
    "                #X_test = np.concatenate((X_test, melgram), axis=0)\n",
    "                #Y_test = np.concatenate((Y_test, this_Y), axis=0)\n",
    "                paths_test.append(audio_path)\n",
    "                test_count += 1\n",
    "        print(\"\")\n",
    "\n",
    "    print(\"Shuffling order of data...\")\n",
    "    X_train, Y_train, paths_train = shuffle_XY_paths(X_train, Y_train, paths_train)\n",
    "    X_test, Y_test, paths_test = shuffle_XY_paths(X_test, Y_test, paths_test)\n",
    "\n",
    "    return X_train, Y_train, paths_train, X_test, Y_test, paths_test, class_names, sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_names =  ['Love, Happiness', 'Sadness, Sorrow']\n",
      "total files =  46\n",
      "os.path.exists(audio_path) True\n",
      "   get_sample_dimensions: melgram.shape =  (1, 1, 96, 157)\n",
      "\n",
      "\r",
      " Loading class: Love, Happiness ( 1 of  2 classes), file 1 of 24: Preproc/Love, Happiness/334490_209_091532__3.wav.npy\n",
      "\n",
      "\n",
      "\r",
      " Loading class: Sadness, Sorrow ( 2 of  2 classes), file 1 of 22: Preproc/Sadness, Sorrow/334491_208_091642__3.wav.npy\n",
      "\n",
      "Shuffling order of data...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# get the data\n",
    "x_train, y_train, paths_train, x_test, y_test, paths_test, class_names, sr = build_datasets(preproc=True)\n",
    "\n",
    "# # input image dimensions\n",
    "img_rows, img_cols = 96, 157\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36, 96, 157, 1) (36, 2) (10, 96, 157, 1) (10, 2)\n",
      "x_train shape: (36, 96, 157, 1)\n",
      "36 train samples\n",
      "10 test samples\n",
      "(36, 2)\n"
     ]
    }
   ],
   "source": [
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n",
    "    \n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "print(y_train.shape)\n",
    "\n",
    "# # convert class vectors to binary class matrices\n",
    "# y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "# y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape (36, 96, 157, 1)\n",
      "y_train.shape (36, 2)\n",
      "x_test.shape (10, 96, 157, 1)\n",
      "y_test.shape (10, 2)\n",
      "input_shape (96, 157, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"x_train.shape\", x_train.shape)\n",
    "print(\"y_train.shape\", y_train.shape)\n",
    "print(\"x_test.shape\", x_test.shape)\n",
    "print(\"y_test.shape\", y_test.shape)\n",
    "# print(\"class_names\", class_names)\n",
    "print(\"input_shape\", input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plt.figure(figsize=(10,10))\n",
    "# start = 9;\n",
    "# plt.subplot(221)\n",
    "# plt.imshow(x_train[start+0,:,:,0])\n",
    "# plt.subplot(222)\n",
    "# plt.imshow(x_train[start+1,:,:,0])\n",
    "# plt.subplot(223)\n",
    "# plt.imshow(x_train[start+2,:,:,0])\n",
    "# plt.subplot(224)\n",
    "# plt.imshow(x_train[start+3,:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36 samples, validate on 10 samples\n",
      "Epoch 1/12\n",
      "36/36 [==============================] - 3s - loss: 0.6933 - acc: 0.3889 - val_loss: 0.7000 - val_acc: 0.5000\n",
      "Epoch 2/12\n",
      "36/36 [==============================] - 1s - loss: 0.6818 - acc: 0.5278 - val_loss: 0.6807 - val_acc: 0.5000\n",
      "Epoch 3/12\n",
      "36/36 [==============================] - 1s - loss: 0.6656 - acc: 0.6111 - val_loss: 0.6646 - val_acc: 0.7000\n",
      "Epoch 4/12\n",
      "36/36 [==============================] - 1s - loss: 0.6325 - acc: 0.7500 - val_loss: 0.6521 - val_acc: 0.6000\n",
      "Epoch 5/12\n",
      "36/36 [==============================] - 1s - loss: 0.6056 - acc: 0.7222 - val_loss: 0.8414 - val_acc: 0.5000\n",
      "Epoch 6/12\n",
      "36/36 [==============================] - 2s - loss: 0.7004 - acc: 0.5556 - val_loss: 0.7364 - val_acc: 0.5000\n",
      "Epoch 7/12\n",
      "36/36 [==============================] - 1s - loss: 0.7422 - acc: 0.4722 - val_loss: 0.6730 - val_acc: 0.5000\n",
      "Epoch 8/12\n",
      "36/36 [==============================] - 1s - loss: 0.6156 - acc: 0.6111 - val_loss: 0.6505 - val_acc: 0.8000\n",
      "Epoch 9/12\n",
      "36/36 [==============================] - 2s - loss: 0.5734 - acc: 0.7500 - val_loss: 0.6345 - val_acc: 0.7000\n",
      "Epoch 10/12\n",
      "36/36 [==============================] - 2s - loss: 0.5256 - acc: 0.7500 - val_loss: 0.6063 - val_acc: 0.6000\n",
      "Epoch 11/12\n",
      "36/36 [==============================] - 1s - loss: 0.4809 - acc: 0.8611 - val_loss: 0.6375 - val_acc: 0.5000\n",
      "Epoch 12/12\n",
      "36/36 [==============================] - 2s - loss: 0.4891 - acc: 0.7778 - val_loss: 0.8186 - val_acc: 0.5000\n",
      "Test loss: 0.818624138832\n",
      "Test accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(class_names), activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "#           validation_split = 0.2\n",
    "          validation_data=(x_test, y_test)\n",
    "         )\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "(60000, 10)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# batch_size = 128\n",
    "# num_classes = 10\n",
    "# epochs = 12\n",
    "\n",
    "# # input image dimensions\n",
    "# img_rows, img_cols = 28, 28\n",
    "\n",
    "# # the data, shuffled and split between train and test sets\n",
    "# (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# if K.image_data_format() == 'channels_first':\n",
    "#     x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "#     x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "#     input_shape = (1, img_rows, img_cols)\n",
    "# else:\n",
    "#     x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "#     x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "#     input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# x_train = x_train.astype('float32')\n",
    "# x_test = x_test.astype('float32')\n",
    "# x_train /= 255\n",
    "# x_test /= 255\n",
    "# print('x_train shape:', x_train.shape)\n",
    "# print(x_train.shape[0], 'train samples')\n",
    "# print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# # convert class vectors to binary class matrices\n",
    "# y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "# y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# print(y_train.shape)\n",
    "# print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = 10\n",
    "cate = 8\n",
    "\n",
    "for i,y_train_data in enumerate(y_train):\n",
    "    if y_train_data[cate] == 1:\n",
    "        count-=1\n",
    "        x_train[i]\n",
    "        plt.figure()\n",
    "        plt.imshow(x_train[i,:,:,0])\n",
    "    if count == 0:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
